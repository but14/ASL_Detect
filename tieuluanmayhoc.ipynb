{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/but14/ASL_Detect/blob/main/tieuluanmayhoc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3FnstMyb762"
      },
      "source": [
        "# K-Nearest Neighbor(KNN) Algorithm algorithm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj3MDey3OEUS"
      },
      "source": [
        "CIFAR-10 là dataset được đánh nhãn do đó đây là máy học có giám sát (Supervised Learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifWEitTP2tn5",
        "outputId": "dde15e42-edf4-4ad6-d00f-947ac0db313b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 33.98%\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import numpy as np\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Flatten the images for KNN\n",
        "X_train_flattened = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test_flattened = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# Normalize the data\n",
        "X_train_flattened = X_train_flattened / 255.0\n",
        "X_test_flattened = X_test_flattened / 255.0\n",
        "\n",
        "# Initialize KNN model\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train KNN on the training data\n",
        "knn.fit(X_train_flattened, y_train.ravel())\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = knn.predict(X_test_flattened)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEvomlCa-PZx",
        "outputId": "f5350b1d-e08d-4abd-9e74-0591e47635b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 24.0 %\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load CIFAR-10 dataset and reduce size\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "X_train, y_train = X_train[:2000].reshape(2000, -1) / 255.0, y_train[:2000]\n",
        "X_test, y_test = X_test[:500].reshape(500, -1) / 255.0, y_test[:500]\n",
        "\n",
        "# Train and evaluate KNN\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train.ravel())\n",
        "print(\"Test accuracy:\", accuracy_score(y_test, knn.predict(X_test)) * 100, \"%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfJWkX0Q_POW"
      },
      "source": [
        "ở đây độ chính xác giảm do n = 3 và KNN test chỉ xữ lí khung hình 500 nên tốc độ nhanh nhưng độ chính xác không cao"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzNaz1z32wBi"
      },
      "source": [
        "#  Principal Component Analysis (PCA) - Dương Nguyễn Anh Quân\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyWWtiwNXivJ",
        "outputId": "5bfe009e-22d9-4a58-d38f-650cb4d1a99f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting grid search...\n",
            "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Flatten the images\n",
        "x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test_flat = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "x_train_flat = scaler.fit_transform(x_train_flat)\n",
        "x_test_flat = scaler.transform(x_test_flat)\n",
        "\n",
        "# Use much more PCA components - getting closer to original dimensions\n",
        "pca = PCA(n_components=2000)  # Significantly increased\n",
        "x_train_pca = pca.fit_transform(x_train_flat)\n",
        "x_test_pca = pca.transform(x_test_flat)\n",
        "\n",
        "# More extensive parameter grid\n",
        "param_grid = {\n",
        "    'n_neighbors': [1, 2, 3, 4, 5],  # Added more neighbor options\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['cosine', 'manhattan'],  # Manhattan distance can work well with high dimensions\n",
        "    'p': [1, 2],  # L1 and L2 distance metrics\n",
        "    'leaf_size': [10, 30, 50]  # Can affect performance for large datasets\n",
        "}\n",
        "\n",
        "# Run grid search with cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    KNeighborsClassifier(n_jobs=-1),\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "print(\"Starting grid search...\")\n",
        "grid_search.fit(x_train_pca, y_train.ravel())\n",
        "\n",
        "# Use best model for final prediction\n",
        "best_knn = grid_search.best_estimator_\n",
        "y_pred = best_knn.predict(x_test_pca)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'\\nFinal Test Accuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYuWIxia3tLC"
      },
      "source": [
        "# Singular Value Decomposition (SVD) - Lê Trần Quốc Bảo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWr_k8Xj4MEV",
        "outputId": "c6354a47-9b68-4bc5-eccc-8073f0d68556"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 0us/step\n",
            "Độ chính xác của mô hình KNN với SVD trên CIFAR-10: 38.02%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "# 1. Tải dữ liệu CIFAR-10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Chuyển đổi dữ liệu thành dạng vector (kết hợp tất cả kênh RGB)\n",
        "x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test_flat = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "# 2. Giảm chiều dữ liệu với SVD\n",
        "n_components = 100  # Số chiều sau khi giảm, có thể thay đổi để tối ưu\n",
        "svd = TruncatedSVD(n_components=n_components)\n",
        "x_train_svd = svd.fit_transform(x_train_flat)\n",
        "x_test_svd = svd.transform(x_test_flat)\n",
        "\n",
        "# 3. Huấn luyện và kiểm thử với KNN\n",
        "k = 5  # Số lượng lân cận (neighbors)\n",
        "knn = KNeighborsClassifier(n_neighbors=k)\n",
        "knn.fit(x_train_svd, y_train.ravel())\n",
        "\n",
        "# Dự đoán và đánh giá độ chính xác\n",
        "y_pred = knn.predict(x_test_svd)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Độ chính xác của mô hình KNN với SVD trên CIFAR-10: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u312YBzv3_mU"
      },
      "source": [
        "#  Feature Engineering and Selection - Lê Nguyên Thành"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfAb5qSvVuZ7",
        "outputId": "3795afef-302e-4b44-b2ed-0a0fe7377798"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
            "Accuracy with k=1: 38.59%\n",
            "Accuracy with k=2: 34.61%\n",
            "Accuracy with k=3: 37.05%\n",
            "Accuracy with k=4: 37.42%\n",
            "Accuracy with k=5: 38.44%\n",
            "Accuracy with k=6: 38.02%\n",
            "Accuracy with k=7: 38.29%\n",
            "Accuracy with k=8: 38.33%\n",
            "Accuracy with k=9: 38.44%\n",
            "Accuracy with k=10: 38.80%\n",
            "Accuracy with k=11: 39.00%\n",
            "Accuracy with k=12: 38.93%\n",
            "Accuracy with k=13: 39.18%\n",
            "Accuracy with k=14: 38.69%\n",
            "Accuracy with k=15: 38.77%\n",
            "Accuracy with k=16: 38.75%\n",
            "Accuracy with k=17: 38.65%\n",
            "Accuracy with k=18: 38.46%\n",
            "Accuracy with k=19: 38.46%\n",
            "Accuracy with k=20: 38.56%\n",
            "Best accuracy with KNN: 39.18% using k=13\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize data to [0, 1]\n",
        "\n",
        "# Feature Engineering - Compute mean and standard deviation of RGB channels\n",
        "def rgb_channel_features(images):\n",
        "    # Calculate mean and standard deviation for each RGB channel\n",
        "    r_mean = np.mean(images[:, :, :, 0], axis=(1, 2))\n",
        "    g_mean = np.mean(images[:, :, :, 1], axis=(1, 2))\n",
        "    b_mean = np.mean(images[:, :, :, 2], axis=(1, 2))\n",
        "    r_std = np.std(images[:, :, :, 0], axis=(1, 2))\n",
        "    g_std = np.std(images[:, :, :, 1], axis=(1, 2))\n",
        "    b_std = np.std(images[:, :, :, 2], axis=(1, 2))\n",
        "    return np.column_stack((r_mean, g_mean, b_mean, r_std, g_std, b_std))\n",
        "\n",
        "# Extract channel statistics as features\n",
        "x_train_features = rgb_channel_features(x_train)\n",
        "x_test_features = rgb_channel_features(x_test)\n",
        "\n",
        "# Reshape images to 2D and concatenate with extracted features\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "x_train = np.hstack((x_train, x_train_features))\n",
        "x_test = np.hstack((x_test, x_test_features))\n",
        "\n",
        "# Step 3: Standardize data\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "# Step 4: Reduce dimensionality using PCA\n",
        "pca = PCA(n_components=100)  # Keep 100 principal components\n",
        "x_train_pca = pca.fit_transform(x_train)\n",
        "x_test_pca = pca.transform(x_test)\n",
        "\n",
        "# Step 5: Tune KNN parameters and find the best k\n",
        "best_accuracy = 0\n",
        "best_k = 0\n",
        "for k in range(1, 21):  # Test for k from 1 to 20\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(x_train_pca, y_train.ravel())\n",
        "    y_pred = knn.predict(x_test_pca)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy with k={k}: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_k = k\n",
        "\n",
        "print(f\"Best accuracy with KNN: {best_accuracy * 100:.2f}% using k={best_k}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxqjMLvP25f4"
      },
      "source": [
        "#  CLIP với K-NN - Phạm Nguyễn Hoàng Long"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES05S4ls47bl",
        "outputId": "fb8a776d-f407-4024-fd9a-5ccf2bec0954"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall accuracy on CIFAR-10: 93.91%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Tải dữ liệu CIFAR-10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "def get_clip_features(images):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Kiểm tra GPU\n",
        "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to(device)  # Chuyển mô hình sang GPU\n",
        "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "\n",
        "\n",
        "    processor.feature_extractor.do_rescale = False\n",
        "\n",
        "    features = []\n",
        "    for img in images:\n",
        "        inputs = processor(images=transform(img), return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model.get_image_features(**inputs)\n",
        "        features.append(output.cpu().numpy().flatten())\n",
        "\n",
        "    return np.array(features)\n",
        "\n",
        "\n",
        "train_features = get_clip_features(x_train)\n",
        "test_features = get_clip_features(x_test)\n",
        "\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=7)\n",
        "knn.fit(train_features, y_train.flatten())\n",
        "\n",
        "\n",
        "y_pred = knn.predict(test_features)\n",
        "\n",
        "# Tính toán độ chính xác\n",
        "accuracy = accuracy_score(y_test.flatten(), y_pred)\n",
        "print(f\"Overall accuracy on CIFAR-10: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ka3IEVEbwF0r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJoyNxBY4ncE"
      },
      "source": [
        "# Data Augmentation - Trịnh Ngọc Nhất"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPMKAMxO4M6Y",
        "outputId": "c10ebac4-a2c7-4956-f367-537d945fb205"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy with data augmentation: 35.57%\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Flatten the images for KNN (before augmentation)\n",
        "X_train_flattened = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test_flattened = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# Normalize the data\n",
        "X_train_flattened = X_train_flattened / 255.0\n",
        "X_test_flattened = X_test_flattened / 255.0\n",
        "\n",
        "# Set up data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,      # Randomly rotate images by 15 degrees\n",
        "    width_shift_range=0.1,  # Shift images horizontally by 10%\n",
        "    height_shift_range=0.1, # Shift images vertically by 10%\n",
        "    horizontal_flip=True    # Randomly flip images horizontally\n",
        ")\n",
        "\n",
        "# Fit the generator on the training data\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Augment data by generating a subset of augmented images for KNN\n",
        "X_augmented, y_augmented = [], []\n",
        "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=5000, shuffle=False):\n",
        "    X_augmented.append(X_batch)\n",
        "    y_augmented.append(y_batch)\n",
        "    if len(X_augmented) * 5000 >= 100000:  # Limit the augmented set to 10,000 samples\n",
        "        break\n",
        "\n",
        "# Convert the augmented data to numpy arrays\n",
        "X_augmented = np.concatenate(X_augmented).reshape(-1, 32 * 32 * 3) / 255.0\n",
        "y_augmented = np.concatenate(y_augmented).ravel()\n",
        "\n",
        "# Combine the original and augmented data\n",
        "X_train_combined = np.concatenate((X_train_flattened, X_augmented), axis=0)\n",
        "y_train_combined = np.concatenate((y_train.ravel(), y_augmented), axis=0)\n",
        "\n",
        "# Initialize KNN model\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train KNN on the combined training data\n",
        "knn.fit(X_train_combined, y_train_combined)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = knn.predict(X_test_flattened)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test accuracy with data augmentation: {accuracy * 100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}